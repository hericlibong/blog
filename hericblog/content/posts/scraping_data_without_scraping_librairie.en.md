---
title: "Easy Data extraction : Hidden API with Scrapy "
date: 2023-03-20T23:41:32Z
draft: false
---



Extracting data from dynamic websites is never easy. You have to know how to manage content generated by Javascript and master asynchronous data loading. You have to know how to bypass possible security measures such as captchas or IP address blocking. It is also necessary to deal with the frequent changes of structure put in place by developers to make extraction rules and scripts obsolete and, among other things, to discourage scraping.
   
Of course, there are many programs, libraries and methods to get around these obstacles. But this requires at least intermediate technical programming skills and time, especially if you are involved in a large collection.

"Collecting data from an internal or hidden API has the advantage of offering a variety of options for choosing the library once it is detected and if the response allows it. Since the data is in JSON format, any tool such as Scrapy, Beautiful Soup or Request can be used to access it."

The variety of options available for library selection is not specifically an advantage of collecting data from a hidden API, but rather an advantage of working with data in JSON format.

However, using a hidden API can have other advantages, such as:

Access to data that is not publicly available, which can provide a competitive advantage to the company or organisation collecting it.
The ability to obtain data in real time or near real time, which can be particularly useful for real-time monitoring or tracking applications.
Reduced data collection costs, as using an API can be cheaper than other data collection methods, such as web scraping.
In addition, collection time is an important advantage of using a hidden API over a traditional scraping method. With an API, data retrieval can be done much faster, which can result in real-time or near real-time data.

In comparison, traditional web scraping can be very time consuming, especially if the data is scattered over several pages or websites. It can also be more difficult to retrieve structured and consistent data with scraping, as websites can change their structure or format unpredictably.

Using a hidden API can therefore significantly reduce the time needed to collect data, as well as ensuring greater consistency and quality of the retrieved data. 


This is a Python code that uses the Scrapy library to scrape data from a website. The purpose of the code is to extract product information from a JSON file located at a specific URL and store it in a structured format. Let's go through the code block by block to see how it works.


```python
import scrapy
import json

```

In this block, we import the Scrapy library and the json module.

```python
class DiashopspiderSpider(scrapy.Spider):
    name = 'diashopspider'
    allowed_domains = ['ylqg8i.a.searchspring.io']
    start_urls = ['https://ylqg8i.a.searchspring.io/api/search/search.json?siteId=ylqg8i&resultsFormat=native&resultsPerPage=28&bgfilter.ss_is_published=1&bgfilter.collection_name=New%20Arrivals&bgfilter.ss_new_45=1&page=1']


```

In this block, we define a new class named "DiashopspiderSpider" that extends the Scrapy Spider class. We set the spider name to "diashopspider", specify the domain we want to scrape, and set the start URL to the specific JSON file that we want to extract data from.


```python

    def parse(self, response):
        parse_json = json.loads(response.body)
        items = parse_json['results'] 
        for item in items:
            yield {
                "Collection": "New Arrival",
                "Brand": item["brand"],
                "Name": item["name"],
                "ssName": item["ss_name"],
                "Price": item["price"],
                "VariantComparePrice": item.get("variant_compare_price", "None"),
                "Msrp": item["msrp"],
                "ProductType": item["product_type_unigram"],
                "Stock": item["ss_instock_pct"],
                "Sku": item["sku"],
                "colorScheme": item.get("tags_dia_color_scheme", "None"),
                "PrimaryColor": item.get("tags_dia_primary_color", "None"),
                "Description": item.get("body_html", "None"),
                "Images": item.get("images", "None"),
                
            }


```

This is the main function of the spider. It receives the response object from the start URL and loads the JSON content of the response body. It then extracts the "results" key from the JSON and loops through each item in the list. For each item, it creates a dictionary containing the product information that we want to extract, such as the brand, name, price, and images. Finally, it uses the "yield" keyword to return the dictionary.



```python

        current_page = parse_json['pagination']['currentPage']
        next_page_check = parse_json['pagination']['nextPage']
        if next_page_check!=0: 
            next_page = current_page + 1
            next_page_url = f'https://ylqg8i.a.searchspring.io/api/search/search.json?siteId=ylqg8i&resultsFormat=native&resultsPerPage=28&bgfilter.ss_is_published=1&bgfilter.collection_name=New%20Arrivals&bgfilter.ss_new_45=1&page={next_page}'
            yield scrapy.Request(url=next_page_url, callback=self.parse)


```

This block is responsible for navigating to the next page of the JSON file if there is one. It extracts the current page number and the "nextPage" value from the JSON pagination object. If the "nextPage" value is not zero, it calculates the URL for the next page and creates a new Scrapy Request object

###

L’extraction de données à partir de sites  web dynamiques n’est jamais chose aisée. 
Il faut  savoir gérer un contenu généré par Javascript et maîtriser le chargement asynchrone des données. il faut savoir  contourner les éventuelles mesures de sécurité tels que les Captchas ou les bloquages d’addresses IP. Il faut également faire avec les fréquents  changements de structure mis en place par les  dévellopeurs afin de rendre les  règles d’extraction et les scripts obsolètes et, entre autres,  décourager les adeptes du scraping.
"La collecte de données à partir d'une API interne ou cachée présente l'avantage de proposer une variété d'options de choix de la bibliothèque une fois que celle-ci est détectée et si la réponse le permet. Étant donné que les données sont au format JSON, n'importe quel outil tel que Scrapy, Beautiful Soup ou Request peut être utilisé pour y accéder."

La variété d'options offertes pour la sélection de la bibliothèque n'est pas spécifiquement un avantage de la collecte de données à partir d'une API cachée, mais plutôt un avantage de travailler avec des données au format JSON.

Cependant, l'utilisation d'une API cachée peut présenter d'autres avantages, tels que :

   -  L'accès à des données qui ne sont pas disponibles publiquement, ce qui peut offrir un avantage concurrentiel à l'entreprise ou à l'organisation qui les collecte.
   -  La possibilité d'obtenir des données en temps réel ou presque en temps réel, ce qui peut être particulièrement utile pour les applications de surveillance ou de suivi en temps réel.
   -  La réduction des coûts de collecte de données, car l'utilisation d'une API peut être moins coûteuse que d'autres méthodes de collecte de données, telles que le scraping web.

De plus le temps de collecte est un avantage important de l'utilisation d'une API cachée par rapport à une méthode de scraping classique. Avec une API, la récupération des données peut être effectuée beaucoup plus rapidement, ce qui peut permettre d'obtenir des données en temps réel ou presque en temps réel.

En comparaison, le scraping web classique peut prendre beaucoup de temps, surtout si les données sont dispersées sur plusieurs pages ou sites web. Il peut également être plus difficile de récupérer des données structurées et cohérentes avec le scraping, car les sites web peuvent changer leur structure ou leur format de manière imprévisible.

L'utilisation d'une API cachée permet donc de réduire considérablement le temps nécessaire pour collecter les données, ainsi que de garantir une plus grande cohérence et une meilleure qualité des données récupérées.